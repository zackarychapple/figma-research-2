import * as dotenv from 'dotenv';
import * as path from 'path';
import { fileURLToPath } from 'url';
import * as fs from 'fs';

// Get __dirname equivalent in ES modules
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Load environment variables
dotenv.config({ path: path.join(__dirname, '..', '.env') });

interface EmbeddingTestResult {
  modelId: string;
  type: 'text' | 'visual' | 'multimodal';
  available: boolean;
  latencyMs?: number;
  dimensions?: number;
  costEstimate?: number;
  error?: string;
  metadata?: any;
}

interface TestReport {
  timestamp: string;
  apiKeyValid: boolean;
  textEmbeddings: EmbeddingTestResult[];
  visualEmbeddings: EmbeddingTestResult[];
  recommendations: string[];
  phase1Comparison: {
    textEmbeddingsStatus: string;
    visualEmbeddingsStatus: string;
    changes: string[];
  };
}

class OpenRouterEmbeddingValidator {
  private apiKey: string;
  private baseUrl = 'https://openrouter.ai/api/v1';
  private results: TestReport;

  constructor() {
    this.apiKey = process.env.OPENROUTER || process.env.OPENROUTER_API_KEY || '';
    if (!this.apiKey) {
      throw new Error('OPENROUTER API key not found in environment');
    }

    this.results = {
      timestamp: new Date().toISOString(),
      apiKeyValid: false,
      textEmbeddings: [],
      visualEmbeddings: [],
      recommendations: [],
      phase1Comparison: {
        textEmbeddingsStatus: 'Testing...',
        visualEmbeddingsStatus: 'Testing...',
        changes: []
      }
    };
  }

  private async makeRequest(endpoint: string, body: any): Promise<any> {
    const url = `${this.baseUrl}${endpoint}`;
    console.log(`\nMaking request to: ${endpoint}`);
    console.log(`Payload:`, JSON.stringify(body, null, 2));

    try {
      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
          'HTTP-Referer': 'https://github.com/zackarychapple/figma-research',
          'X-Title': 'Figma-to-Code Validation'
        },
        body: JSON.stringify(body)
      });

      console.log(`Response status: ${response.status} ${response.statusText}`);

      const responseText = await response.text();
      console.log(`Response body (first 500 chars): ${responseText.substring(0, 500)}`);

      if (!response.ok) {
        throw new Error(`API error: ${response.status} ${response.statusText} - ${responseText}`);
      }

      return JSON.parse(responseText);
    } catch (error: any) {
      console.error(`Request failed:`, error.message);
      throw error;
    }
  }

  async testTextEmbeddings(): Promise<void> {
    console.log('\n' + '='.repeat(80));
    console.log('TESTING TEXT EMBEDDING MODELS');
    console.log('='.repeat(80));

    const textModelsToTest = [
      'openai/text-embedding-3-small',
      'openai/text-embedding-3-large',
      'openai/text-embedding-ada-002',
      'text-embedding-3-small',
      'text-embedding-3-large'
    ];

    const testInput = 'A blue button with rounded corners and white text saying "Submit"';

    for (const modelId of textModelsToTest) {
      console.log(`\n${'‚îÄ'.repeat(80)}`);
      console.log(`Testing text embedding model: ${modelId}`);
      console.log(`${'‚îÄ'.repeat(80)}`);

      const startTime = Date.now();

      try {
        const response = await this.makeRequest('/embeddings', {
          model: modelId,
          input: testInput
        });

        const latency = Date.now() - startTime;
        const embedding = response.data?.[0]?.embedding;
        const dimensions = embedding?.length || 0;

        // Estimate cost (OpenAI pricing through OpenRouter)
        const tokensUsed = Math.ceil(testInput.split(' ').length * 1.3);
        const costEstimate = (tokensUsed / 1000000) * 0.02; // $0.02 per 1M tokens for small

        this.results.textEmbeddings.push({
          modelId,
          type: 'text',
          available: true,
          latencyMs: latency,
          dimensions,
          costEstimate,
          metadata: {
            tokensUsed,
            embeddingPreview: embedding?.slice(0, 5),
            responseObject: response.object
          }
        });

        console.log(`‚úÖ SUCCESS`);
        console.log(`   Latency: ${latency}ms`);
        console.log(`   Dimensions: ${dimensions}`);
        console.log(`   Cost: $${costEstimate.toFixed(8)}`);
        console.log(`   Embedding sample: [${embedding?.slice(0, 5).join(', ')}...]`);

      } catch (error: any) {
        console.log(`‚ùå FAILED: ${error.message}`);

        this.results.textEmbeddings.push({
          modelId,
          type: 'text',
          available: false,
          error: error.message
        });
      }
    }
  }

  async testVisualEmbeddings(): Promise<void> {
    console.log('\n' + '='.repeat(80));
    console.log('TESTING VISUAL EMBEDDING MODELS');
    console.log('='.repeat(80));

    const visualModelsToTest = [
      // CLIP models
      'openai/clip-vit-large-patch14',
      'openai/clip-vit-base-patch32',
      'clip-vit-large-patch14',
      'clip-vit-base-patch32',
      // Other potential visual embedding models
      'google/gemini-embedding-001',
      'voyage/voyage-multimodal-3',
      'nomic/nomic-embed-vision-v1.5',
      // Try generic vision endpoints
      'vision-embedding',
      'image-embedding'
    ];

    // Test with a simple image URL and base64 data
    const testImageUrl = 'https://via.placeholder.com/150/0000FF/FFFFFF?text=Button';

    // Create a simple test base64 image (1x1 blue pixel PNG)
    const testBase64 = 'iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNkYPhfDwAChwGA60e6kgAAAABJRU5ErkJggg==';

    for (const modelId of visualModelsToTest) {
      console.log(`\n${'‚îÄ'.repeat(80)}`);
      console.log(`Testing visual embedding model: ${modelId}`);
      console.log(`${'‚îÄ'.repeat(80)}`);

      // Try multiple input formats
      const inputFormats = [
        { name: 'Image URL', input: testImageUrl },
        { name: 'Base64 Data URI', input: `data:image/png;base64,${testBase64}` },
        { name: 'Base64 Plain', input: testBase64 }
      ];

      let succeeded = false;

      for (const format of inputFormats) {
        if (succeeded) break;

        console.log(`   Trying format: ${format.name}`);
        const startTime = Date.now();

        try {
          const response = await this.makeRequest('/embeddings', {
            model: modelId,
            input: format.input
          });

          const latency = Date.now() - startTime;
          const embedding = response.data?.[0]?.embedding;
          const dimensions = embedding?.length || 0;

          this.results.visualEmbeddings.push({
            modelId,
            type: 'visual',
            available: true,
            latencyMs: latency,
            dimensions,
            metadata: {
              inputFormat: format.name,
              embeddingPreview: embedding?.slice(0, 5),
              responseObject: response.object
            }
          });

          console.log(`   ‚úÖ SUCCESS with ${format.name}`);
          console.log(`      Latency: ${latency}ms`);
          console.log(`      Dimensions: ${dimensions}`);
          console.log(`      Embedding sample: [${embedding?.slice(0, 5).join(', ')}...]`);

          succeeded = true;

        } catch (error: any) {
          console.log(`   ‚ùå Failed with ${format.name}: ${error.message.substring(0, 100)}`);
        }
      }

      if (!succeeded) {
        this.results.visualEmbeddings.push({
          modelId,
          type: 'visual',
          available: false,
          error: 'All input formats failed'
        });
      }
    }
  }

  async testAvailableModels(): Promise<void> {
    console.log('\n' + '='.repeat(80));
    console.log('QUERYING AVAILABLE MODELS LIST');
    console.log('='.repeat(80));

    try {
      const response = await fetch('https://openrouter.ai/api/v1/models', {
        method: 'GET',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        }
      });

      if (!response.ok) {
        throw new Error(`Failed to fetch models: ${response.status} ${response.statusText}`);
      }

      const data = await response.json();
      const models = data.data || [];

      console.log(`\nTotal models available: ${models.length}`);

      // Filter for embedding-related models
      const embeddingModels = models.filter((m: any) => {
        const id = m.id?.toLowerCase() || '';
        const name = m.name?.toLowerCase() || '';
        const description = m.description?.toLowerCase() || '';

        return id.includes('embed') ||
               id.includes('clip') ||
               name.includes('embed') ||
               name.includes('clip') ||
               description.includes('embed') ||
               description.includes('clip');
      });

      console.log(`\nEmbedding-related models found: ${embeddingModels.length}`);

      if (embeddingModels.length > 0) {
        console.log('\nEmbedding Models:');
        embeddingModels.forEach((model: any) => {
          console.log(`  - ${model.id}`);
          console.log(`    Name: ${model.name}`);
          if (model.description) {
            console.log(`    Description: ${model.description.substring(0, 100)}...`);
          }
          if (model.pricing) {
            console.log(`    Pricing: $${model.pricing.prompt} / $${model.pricing.completion} per 1M tokens`);
          }
        });

        // Save the embedding models list
        fs.writeFileSync(
          path.join(__dirname, 'reports', 'embedding-models-available.json'),
          JSON.stringify(embeddingModels, null, 2)
        );
      } else {
        console.log('\n‚ö†Ô∏è  No embedding models found in the models list');
      }

    } catch (error: any) {
      console.error(`\n‚ùå Failed to query models: ${error.message}`);
    }
  }

  generateReport(): void {
    console.log('\n' + '='.repeat(80));
    console.log('VALIDATION REPORT');
    console.log('='.repeat(80));

    // Analyze results
    const workingTextModels = this.results.textEmbeddings.filter(r => r.available);
    const workingVisualModels = this.results.visualEmbeddings.filter(r => r.available);

    console.log('\nüìä SUMMARY:');
    console.log(`   Text Embedding Models: ${workingTextModels.length}/${this.results.textEmbeddings.length} working`);
    console.log(`   Visual Embedding Models: ${workingVisualModels.length}/${this.results.visualEmbeddings.length} working`);

    // Phase 1 comparison
    console.log('\nüîÑ COMPARISON TO PHASE 1:');

    if (workingTextModels.length > 0) {
      console.log('   Text Embeddings: ‚úÖ STILL AVAILABLE (unchanged from Phase 1)');
      this.results.phase1Comparison.textEmbeddingsStatus = 'AVAILABLE - No change';

      const avgLatency = workingTextModels.reduce((sum, r) => sum + (r.latencyMs || 0), 0) / workingTextModels.length;
      console.log(`   Average latency: ${avgLatency.toFixed(0)}ms`);
    } else {
      console.log('   Text Embeddings: ‚ùå NOT AVAILABLE (changed from Phase 1!)');
      this.results.phase1Comparison.textEmbeddingsStatus = 'NOT AVAILABLE - REGRESSION';
      this.results.phase1Comparison.changes.push('Text embeddings no longer available');
    }

    if (workingVisualModels.length > 0) {
      console.log('   Visual Embeddings: ‚úÖ NOW AVAILABLE (NEW! Changed from Phase 1)');
      this.results.phase1Comparison.visualEmbeddingsStatus = 'NOW AVAILABLE - Major improvement!';
      this.results.phase1Comparison.changes.push('Visual embeddings now supported');
    } else {
      console.log('   Visual Embeddings: ‚ùå NOT AVAILABLE (unchanged from Phase 1)');
      this.results.phase1Comparison.visualEmbeddingsStatus = 'NOT AVAILABLE - No change';
    }

    // Recommendations
    console.log('\nüí° RECOMMENDATIONS:');

    if (workingTextModels.length > 0 && workingVisualModels.length > 0) {
      this.results.recommendations.push('‚úÖ Use OpenRouter for ALL embeddings (text + visual)');
      this.results.recommendations.push('‚úÖ Remove OpenAI direct API dependency');
      this.results.recommendations.push('‚úÖ Simplify architecture to single API key');
      console.log('   ‚úÖ Use OpenRouter for ALL embeddings (text + visual)');
      console.log('   ‚úÖ Remove OpenAI direct API dependency');
      console.log('   ‚úÖ Simplify architecture to single API key');
    } else if (workingTextModels.length > 0 && workingVisualModels.length === 0) {
      this.results.recommendations.push('‚ö†Ô∏è  Use OpenRouter for text embeddings only');
      this.results.recommendations.push('‚ö†Ô∏è  Keep OpenAI direct API for visual embeddings (CLIP)');
      this.results.recommendations.push('‚ÑπÔ∏è  No architecture change from Phase 1');
      console.log('   ‚ö†Ô∏è  Use OpenRouter for text embeddings only');
      console.log('   ‚ö†Ô∏è  Keep OpenAI direct API for visual embeddings (CLIP)');
      console.log('   ‚ÑπÔ∏è  No architecture change from Phase 1');
    } else {
      this.results.recommendations.push('‚ùå OpenRouter embeddings not working as expected');
      this.results.recommendations.push('üîç Investigate API changes or contact OpenRouter support');
      console.log('   ‚ùå OpenRouter embeddings not working as expected');
      console.log('   üîç Investigate API changes or contact OpenRouter support');
    }

    // Working models
    if (workingTextModels.length > 0) {
      console.log('\n‚úÖ WORKING TEXT MODELS:');
      workingTextModels.forEach(model => {
        console.log(`   - ${model.modelId}`);
        console.log(`     Dimensions: ${model.dimensions}, Latency: ${model.latencyMs}ms, Cost: $${model.costEstimate?.toFixed(8)}`);
      });
    }

    if (workingVisualModels.length > 0) {
      console.log('\n‚úÖ WORKING VISUAL MODELS:');
      workingVisualModels.forEach(model => {
        console.log(`   - ${model.modelId}`);
        console.log(`     Dimensions: ${model.dimensions}, Latency: ${model.latencyMs}ms`);
      });
    }

    // Save report
    const reportPath = path.join(__dirname, 'reports', 'openrouter-embeddings-phase3.md');
    const jsonPath = path.join(__dirname, 'reports', 'openrouter-embeddings-phase3.json');

    // Generate markdown report
    const markdownReport = this.generateMarkdownReport();
    fs.writeFileSync(reportPath, markdownReport);

    // Save JSON data
    fs.writeFileSync(jsonPath, JSON.stringify(this.results, null, 2));

    console.log(`\nüìÑ Full report saved to: ${reportPath}`);
    console.log(`üìä JSON data saved to: ${jsonPath}`);
  }

  private generateMarkdownReport(): string {
    const workingTextModels = this.results.textEmbeddings.filter(r => r.available);
    const workingVisualModels = this.results.visualEmbeddings.filter(r => r.available);

    return `# OpenRouter Embedding Models Validation - Phase 3

**Date:** ${new Date(this.results.timestamp).toLocaleDateString()}
**Task:** task-14.13 - Validate OpenRouter Embedding Models
**Status:** ${workingTextModels.length > 0 || workingVisualModels.length > 0 ? '‚úÖ Complete' : '‚ö†Ô∏è Issues Found'}

---

## Executive Summary

This validation tests OpenRouter's current embedding capabilities following user reports that embeddings are now supported. Phase 1 (Nov 2025) found text embeddings available but visual embeddings missing.

### Key Findings

- **Text Embeddings:** ${workingTextModels.length}/${this.results.textEmbeddings.length} models working
- **Visual Embeddings:** ${workingVisualModels.length}/${this.results.visualEmbeddings.length} models working
- **API Authentication:** ${this.results.apiKeyValid ? '‚úÖ Valid' : '‚ùå Invalid'}

### Phase 1 Comparison

**Text Embeddings:**
- Phase 1: ‚úÖ Available via OpenRouter
- Phase 3: ${this.results.phase1Comparison.textEmbeddingsStatus}

**Visual Embeddings:**
- Phase 1: ‚ùå Not available (required OpenAI direct API)
- Phase 3: ${this.results.phase1Comparison.visualEmbeddingsStatus}

### Recommendations

${this.results.recommendations.map(r => `- ${r}`).join('\n')}

---

## Test Results

### Text Embedding Models

${this.results.textEmbeddings.map(result => {
  if (result.available) {
    return `#### ‚úÖ ${result.modelId}
- **Status:** Available
- **Latency:** ${result.latencyMs}ms
- **Dimensions:** ${result.dimensions}
- **Cost per operation:** $${result.costEstimate?.toFixed(8)}
- **Tokens used:** ${result.metadata?.tokensUsed}
`;
  } else {
    return `#### ‚ùå ${result.modelId}
- **Status:** Not Available
- **Error:** ${result.error}
`;
  }
}).join('\n')}

### Visual Embedding Models

${this.results.visualEmbeddings.map(result => {
  if (result.available) {
    return `#### ‚úÖ ${result.modelId}
- **Status:** Available
- **Latency:** ${result.latencyMs}ms
- **Dimensions:** ${result.dimensions}
- **Input format:** ${result.metadata?.inputFormat}
`;
  } else {
    return `#### ‚ùå ${result.modelId}
- **Status:** Not Available
- **Error:** ${result.error}
`;
  }
}).join('\n')}

---

## Performance Analysis

${workingTextModels.length > 0 ? `
### Text Embeddings Performance

| Model | Latency | Dimensions | Cost |
|-------|---------|------------|------|
${workingTextModels.map(m => `| ${m.modelId} | ${m.latencyMs}ms | ${m.dimensions} | $${m.costEstimate?.toFixed(8)} |`).join('\n')}

**Average Latency:** ${(workingTextModels.reduce((sum, r) => sum + (r.latencyMs || 0), 0) / workingTextModels.length).toFixed(0)}ms
` : '**No working text embedding models found.**'}

${workingVisualModels.length > 0 ? `
### Visual Embeddings Performance

| Model | Latency | Dimensions |
|-------|---------|------------|
${workingVisualModels.map(m => `| ${m.modelId} | ${m.latencyMs}ms | ${m.dimensions} |`).join('\n')}

**Average Latency:** ${(workingVisualModels.reduce((sum, r) => sum + (r.latencyMs || 0), 0) / workingVisualModels.length).toFixed(0)}ms
` : '**No working visual embedding models found.**'}

---

## Architecture Impact

${workingVisualModels.length > 0 ? `
### ‚úÖ Simplified Architecture Available

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Figma-to-Code System (Unified)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚îî‚îÄ‚ñ∫ OpenRouter (Single API Key)
                ‚îú‚îÄ‚ñ∫ Code Generation (Claude)
                ‚îú‚îÄ‚ñ∫ Text Embeddings (${workingTextModels[0]?.modelId || 'N/A'})
                ‚îî‚îÄ‚ñ∫ Visual Embeddings (${workingVisualModels[0]?.modelId || 'N/A'})
\`\`\`

**Benefits:**
- Single API key for all operations
- Simplified credential management
- Unified billing and cost tracking
- No need for OpenAI direct API

**Action Required:**
- Update architecture to remove OpenAI direct dependency
- Update cost projections
- Simplify integration code
` : `
### ‚ö†Ô∏è Hybrid Architecture Required (No Change from Phase 1)

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Figma-to-Code System            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚îú‚îÄ‚ñ∫ OpenRouter
            ‚îÇ   ‚îú‚îÄ‚ñ∫ Code Generation (Claude)
            ‚îÇ   ‚îî‚îÄ‚ñ∫ Text Embeddings (${workingTextModels[0]?.modelId || 'N/A'})
            ‚îÇ
            ‚îî‚îÄ‚ñ∫ OpenAI Direct API
                ‚îî‚îÄ‚ñ∫ Visual Embeddings (CLIP)
\`\`\`

**No changes from Phase 1 architecture required.**
`}

---

## Cost Analysis

${workingTextModels.length > 0 ? `
### Text Embedding Costs

Based on test results:
- Average cost per embedding: $${(workingTextModels.reduce((sum, r) => sum + (r.costEstimate || 0), 0) / workingTextModels.length).toFixed(8)}
- Estimated monthly cost (300 components): $${((workingTextModels.reduce((sum, r) => sum + (r.costEstimate || 0), 0) / workingTextModels.length) * 300).toFixed(4)}
- Estimated annual cost: $${((workingTextModels.reduce((sum, r) => sum + (r.costEstimate || 0), 0) / workingTextModels.length) * 300 * 12).toFixed(2)}
` : ''}

${workingVisualModels.length > 0 ? `
### Visual Embedding Costs

Visual embedding pricing through OpenRouter needs to be determined through actual usage.
` : `
### Visual Embedding Costs (OpenAI Direct)

If visual embeddings remain on OpenAI direct API:
- Estimated cost per image: ~$0.00005
- Monthly cost (300 components): ~$0.015
- Annual cost: ~$0.18
`}

---

## Conclusion

${workingTextModels.length > 0 && workingVisualModels.length > 0 ? `
### ‚úÖ MAJOR IMPROVEMENT - Visual Embeddings Now Available

OpenRouter now supports both text and visual embeddings, allowing for a **simplified single-API architecture**. This is a significant improvement from Phase 1.

**Next Steps:**
1. Update system architecture to use OpenRouter exclusively
2. Remove OpenAI direct API integration
3. Update cost projections and documentation
4. Test embedding quality with real component data
5. Update backlog task status to DONE
` : workingTextModels.length > 0 ? `
### ‚ö†Ô∏è NO CHANGE FROM PHASE 1

Text embeddings work via OpenRouter, but visual embeddings are still not available. The architecture remains unchanged from Phase 1 with OpenAI direct API required for CLIP embeddings.

**Next Steps:**
1. Continue using hybrid architecture (OpenRouter + OpenAI)
2. Monitor OpenRouter for future visual embedding support
3. Consider alternatives if visual quality is critical
` : `
### ‚ùå ISSUES DETECTED

Embeddings are not working as expected through OpenRouter. Further investigation required.

**Next Steps:**
1. Review OpenRouter API documentation updates
2. Contact OpenRouter support for clarification
3. Verify API key permissions
4. Test with updated SDK versions
`}

---

**Report Generated:** ${new Date().toISOString()}
**Validator:** Claude Sonnet 4.5
`;
  }

  async run(): Promise<void> {
    console.log('\n' + '‚ñà'.repeat(80));
    console.log('OPENROUTER EMBEDDING VALIDATION - PHASE 3');
    console.log('Task: task-14.13 - Validate OpenRouter Embedding Models');
    console.log('‚ñà'.repeat(80));

    try {
      this.results.apiKeyValid = true;

      // Query available models first
      await this.testAvailableModels();

      // Test text embeddings
      await this.testTextEmbeddings();

      // Test visual embeddings
      await this.testVisualEmbeddings();

      // Generate report
      this.generateReport();

      console.log('\n‚úÖ Validation complete!');

    } catch (error: any) {
      console.error('\n‚ùå Validation failed:', error.message);
      throw error;
    }
  }
}

// Run validation
const validator = new OpenRouterEmbeddingValidator();
validator.run().catch(console.error);
